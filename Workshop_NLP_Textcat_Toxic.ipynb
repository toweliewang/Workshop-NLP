{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4rc1"
    },
    "colab": {
      "name": "Workshop-NLP-Textcat-Toxic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toweliewang/Workshop-NLP/blob/master/Workshop_NLP_Textcat_Toxic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ9R8JoE1jse",
        "colab_type": "code",
        "outputId": "21d35778-9b6d-451b-e471-32e6e7775044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eokW9-9hobx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "e8ef86be-eec7-4e78-df19-10ab9692c0c8"
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan 22 13:55:16 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36zBQULEhosJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "160ecf3f-7795-4a64-f308-80db4f068551"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/tsundoku-master/empty/kaggle/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/tsundoku-master/empty/kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7aT3v701H-a",
        "colab_type": "code",
        "outputId": "c2663ba3-aff4-46b0-abf3-bd413fa633e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%%html\n",
        "<style> table {float:left} </style>"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style> table {float:left} </style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw7H5bFa1H_G",
        "colab_type": "code",
        "outputId": "54ae297f-5d24-4d8e-85c1-179ad97c323f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        }
      },
      "source": [
        "!pip install torch tqdm lazyme nltk gensim\n",
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Collecting lazyme\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/8c/3bd9a3100a0a702d10fccf60b8ad1007674ea0e2b834dc15314d74f4af8b/lazyme-0.0.23.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.10.47)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.47)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->smart-open>=1.8.1->gensim) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Building wheels for collected packages: lazyme\n",
            "  Building wheel for lazyme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lazyme: filename=lazyme-0.0.23-cp36-none-any.whl size=7931 sha256=5e796c3c37713ccdbf47b68bf979a6e0b1e4f7a49fbca5655f74323777e039bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/0a/0c/0d199c54c031fd72b1914b7348ffcb7676e6e5caac6a84846d\n",
            "Successfully built lazyme\n",
            "Installing collected packages: lazyme\n",
            "Successfully installed lazyme-0.0.23\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHvegMR01H_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdNP1NWL1H_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try: # Use the default NLTK tokenizer.\n",
        "    from nltk import word_tokenize, sent_tokenize \n",
        "    # Testing whether it works. \n",
        "    # Sometimes it doesn't work on some machines because of setup issues.\n",
        "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
        "except: # Use a naive sentence tokenizer and toktok.\n",
        "    import re\n",
        "    from nltk.tokenize import ToktokTokenizer\n",
        "    # See https://stackoverflow.com/a/25736515/610569\n",
        "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
        "    # Use the toktok tokenizer that requires no dependencies.\n",
        "    toktok = ToktokTokenizer()\n",
        "    word_tokenize = word_tokenize = toktok.tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z91iSdJZ1H_g",
        "colab_type": "text"
      },
      "source": [
        "# Classifying Toxic Comments\n",
        "\n",
        "Lets apply what we learnt in a realistic task and **fight cyber-abuse with NLP**!\n",
        "\n",
        "From https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/\n",
        "\n",
        "> *The threat of abuse and harassment online means that many people stop <br>*\n",
        "> *expressing themselves and give up on seeking different opinions. <br>*\n",
        "> *Platforms struggle to effectively facilitate conversations, leading many <br>*\n",
        "> *communities to limit or completely shut down user comments.*\n",
        "\n",
        "\n",
        "The goal of the task is to build a model to detect different types of of toxicity:\n",
        "\n",
        " - toxic\n",
        " - severe toxic\n",
        " - threats\n",
        " - obscenity\n",
        " - insults\n",
        " - identity-based hate\n",
        " \n",
        "In this part, you'll be munging the data as how I would be doing it at work. \n",
        "\n",
        "Your task is to train a feed-forward network on the toxic comments given the skills we have accomplished thus far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZwcrL1w1H_k",
        "colab_type": "text"
      },
      "source": [
        "## Digging into the data...\n",
        "\n",
        "If you're using linux/Mac you can use these bang commands in the notebook:\n",
        "\n",
        "```\n",
        "!pip3 install kaggle\n",
        "!mkdir -p /content/.kaggle/\n",
        "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > /content/.kaggle/kaggle.json\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
        "!unzip /content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/*\n",
        "```\n",
        "\n",
        "Otherwise, download the data from https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHArHFaw1H_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install kaggle\n",
        "# !mkdir -p /content/.kaggle/\n",
        "# !echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > /content/.kaggle/kaggle.json\n",
        "# !chmod 600 /content/.kaggle/kaggle.json\n",
        "# !kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
        "# !unzip /content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQSI3yRM1H_s",
        "colab_type": "code",
        "outputId": "26d16218-5bdd-4ede-e900-c122e22b969e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/tsundoku-master/empty/kaggle/train.csv')\n",
        "df_train.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJoIhuh21IAC",
        "colab_type": "code",
        "outputId": "9f663f3d-cd30-4fdb-d990-ed9a33907372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "df_train[df_train['threat'] == 1]['comment_text']"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79        Hi! I am back again!\\nLast warning!\\nStop undo...\n",
              "176       I think that your a Fagget get a oife and burn...\n",
              "600       I'm also a sock puppet of this account...SUPRI...\n",
              "802       Fuck you, Smith. Please have me notified when ...\n",
              "1017      WOULDN'T BE THE FIRST TIME BITCH. FUCK YOU I'L...\n",
              "                                ...                        \n",
              "157718    bitch \\nyou are a fucking hore. you suck dick ...\n",
              "158717    stupid head \\n\\nYOur dumb and you are stupid d...\n",
              "158856    Hey \\n\\nhey faggot, are you dead yet? or are y...\n",
              "159029                                  Death to Musulmans!\n",
              "159400    Shalom \\n\\nSemite, get the fuck out of here. I...\n",
              "Name: comment_text, Length: 478, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3-EpDMm1IAR",
        "colab_type": "code",
        "outputId": "b20375fe-5c0e-4ae3-8dc5-d10fd46a014a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "df_train.iloc[3712]['comment_text']"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Please stop. If you continue to ignore our policies by introducing inappropriate pages to Wikipedia, you will be blocked.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "107OdYHs1IAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train['comment_text_tokenzied'] = df_train['comment_text'].apply(word_tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luy4SH0Y1IAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from turicreate import SFrame\n",
        "# sf_train = SFrame.read_csv('train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "BpQ0qJjJ1IBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just in case your Jupyter kernel dies, save the tokenized text =)\n",
        "\n",
        "# To save your tokenized text you can do this:\n",
        "import pickle\n",
        "with open('train_tokenized_text.pkl', 'wb') as fout:\n",
        "    pickle.dump(df_train['comment_text_tokenzied'], fout)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmH85Hl_1IBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To load it back:\n",
        "import pickle\n",
        "with open('train_tokenized_text.pkl', 'rb') as fin:\n",
        "    df_train['comment_text_tokenzied'] = pickle.load(fin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZm3Bjqi1IBy",
        "colab_type": "text"
      },
      "source": [
        "# How to get a one-hot?\n",
        "\n",
        "There are many variants of how to get your one-hot embeddings from the individual columns.\n",
        "\n",
        "This is one way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoDz9egy1IB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "c3ad353d-a910-4e75-b288-094a3c454569"
      },
      "source": [
        "label_column_names = \"toxic\tsevere_toxic\tobscene\tthreat\tinsult\tidentity_hate\".split()\n",
        "df_train[label_column_names].values"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDH2JJ0B1ICA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "f3200822-7e5f-4594-e68a-915d679a7566"
      },
      "source": [
        "torch.tensor(df_train[label_column_names].values).float()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNCcngqs1ICW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee93e844-a8b7-4dce-96a0-90349d776ca2"
      },
      "source": [
        "# Convert one-hot to indices of the column.\n",
        "\n",
        "print(np.argmax(df_train[label_column_names].values, axis=1))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "P7xp6Z571ICi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ToxicDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.vocab = Dictionary(texts)\n",
        "        special_tokens = {'<pad>': 0, '<unk>':1}\n",
        "        self.vocab = Dictionary(texts)\n",
        "        self.vocab.patch_with_special_tokens(special_tokens)\n",
        "        \n",
        "        self.vocab_size = len(self.vocab)\n",
        "        \n",
        "        # Vectorize labels\n",
        "        self.labels = torch.tensor(labels)\n",
        "        # Keep track of how many data points.\n",
        "        self._len = len(texts)\n",
        "        \n",
        "        # Find the longest text in the data.\n",
        "        self.max_len = max(len(txt) for txt in texts)\n",
        "        \n",
        "        self.num_labels = len(labels[0])\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        vectorized_sent = self.vectorize(self.texts[index])\n",
        "        # To pad the sentence:\n",
        "        # Pad left = 0; Pad right = max_len - len of sent.\n",
        "        pad_dim = (0, self.max_len - len(vectorized_sent))\n",
        "        vectorized_sent_padded = F.pad(vectorized_sent, pad_dim, 'constant')\n",
        "        return {'x':vectorized_sent_padded, \n",
        "                'y':self.labels[index], \n",
        "                'x_len':len(vectorized_sent)}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "    \n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        # Lets just cast list of indices into torch tensors directly =)\n",
        "        return torch.tensor(self.vocab.doc2idx(tokens))\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjMSYySE1ICn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8bc6dea9-bfe7-4816-c34d-5dc426f8e875"
      },
      "source": [
        "# !pip install --upgrade gensim\n",
        "import gensim\n",
        "gensim.__version__"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.8.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "UIoZOvLz1IC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_column_names = \"toxic\tsevere_toxic\tobscene\tthreat\tinsult\tidentity_hate\".split()\n",
        "toxic_data = ToxicDataset(df_train['comment_text_tokenzied'],\n",
        "                          df_train[label_column_names].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaHsQvLL1IDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "54442288-d736-43d9-fafc-65d3ed0a3cdb"
      },
      "source": [
        "print(toxic_data.texts[123])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Should', 'say', 'something', 'about', 'his', 'views', 'as', 'an', 'educationalist', 'and', 'socialist', 'political', 'commentator', '.', 'Link', 'to', 'http', ':', '//www.langandlit.ualberta.ca/Fall2004/SteigelBainbridge.html', 'mentions', 'this', 'a', 'bit', '-', 'he', 'stood', 'as', 'an', 'election', 'candidate', 'for', 'Respect', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JphmSaok1ID0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6bc5a4c-2d82-494c-95f9-3ffd0bd3c796"
      },
      "source": [
        "len(toxic_data)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1fsjn4K1IEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50\n",
        "dataloader = DataLoader(dataset=toxic_data, \n",
        "                        batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy7GZJEf1IEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "85ed47af-348f-480f-f36f-ec1319e891ba"
      },
      "source": [
        "next(iter(dataloader))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': tensor([[   103,    827,    295,  ...,      0,      0,      0],\n",
              "         [   245,    123,   1121,  ...,      0,      0,      0],\n",
              "         [   103,    596,     38,  ...,      0,      0,      0],\n",
              "         ...,\n",
              "         [   103,     11,    230,  ...,      0,      0,      0],\n",
              "         [  4550,     78,    173,  ...,      0,      0,      0],\n",
              "         [ 68260, 304750,    613,  ...,      0,      0,      0]]),\n",
              " 'x_len': tensor([ 30, 237, 129,  64, 777, 203, 132, 117,  51,   8, 158,  17,   7,  24,\n",
              "          15, 124,   5, 178,  43,  22,  13, 129,  30, 208, 415,  26,  43, 109,\n",
              "          17,  70,  34, 194,  32,  20, 203,  58, 368, 101,  36,  92,  59,  25,\n",
              "          23,  20,  57,  21,   7,  13,  89,  12, 154,  51, 327,  71,  32,  13,\n",
              "         207,  71, 101, 108,  25,  39,  34,  59,  41,  44,  36, 322,  14, 139,\n",
              "          29,  57,  62, 153,  22,  51,  24,  65,  49,  42, 115,  29, 207,  18,\n",
              "          74,  26, 117,  33, 131,  42,   8,   9, 120,  23,  26, 123,  44,  28,\n",
              "          37,  66,  26,  75,  30,  17,  99,  14, 107,  41,  85,  23,  70,   6,\n",
              "          63,  28,  26,  56,  18,  10,  12, 270,  56,  17, 229,  13,  18,  60,\n",
              "          69,  59, 235,  42, 101,  43,   9,  53, 185,  36,  60,  13,  41,  61,\n",
              "         152,  26,  16,  33, 100,  31, 262,  18, 266,  72,  53, 113,  14,  78,\n",
              "         326,   6,  15,  37,  84,  16,  93, 203,  17,  30, 131,  28, 123,  59,\n",
              "          55,  32,  22,  69,  17,  42,  79,  30, 112, 134, 140,  31,   7,  96,\n",
              "          56,   8, 131,  69,  21,  48,  21,  14,  66,  36,  95,  23, 271,  27,\n",
              "         133,  35,  36, 106]),\n",
              " 'y': tensor([[0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeQOBot21IEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFNet(nn.Module):\n",
        "    def __init__(self, max_len, num_labels, vocab_size, embedding_size, hidden_dim):\n",
        "        super(FFNet, self).__init__()\n",
        "        self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                       embedding_dim=embedding_size, \n",
        "                                       padding_idx=0)\n",
        "        # The no. of inputs to the linear layer is the \n",
        "        # no. of tokens in each input * embedding_size\n",
        "        self.linear1 = nn.Linear(embedding_size*max_len, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, num_labels)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        # We want to flatten the inputs so that we get the matrix of shape.\n",
        "        # batch_size x no. of tokens in each input * embedding_size\n",
        "        batch_size, max_len = inputs.shape\n",
        "        embedded = self.embeddings(inputs).view(batch_size, -1)\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        out = self.linear2(hid)\n",
        "        return torch.sigmoid(out)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPUroyAO1IEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9998d62d-b5be-46bd-b9b8-8cef56d07b70"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "embedding_size = 100\n",
        "learning_rate = 0.003\n",
        "hidden_size = 100\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "# Hint: the CBOW model object you've created.\n",
        "model = FFNet(toxic_data.max_len, \n",
        "              len(label_column_names),\n",
        "              toxic_data.vocab_size, \n",
        "              embedding_size=embedding_size, \n",
        "              hidden_dim=hidden_size).to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#model = nn.DataParallel(model)\n",
        "\n",
        "training_losses = []\n",
        "num_epochs = 2\n",
        "for _e in range(num_epochs):\n",
        "    epoch_loss = []\n",
        "    for batch in tqdm(dataloader):\n",
        "        x = batch['x'].to(device)\n",
        "        y = batch['y'].to(device)\n",
        "        # Zero gradient.\n",
        "        optimizer.zero_grad()\n",
        "        # Feed forward.\n",
        "        predictions = model(x)\n",
        "        loss = criterion(predictions, y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss.append(float(loss))\n",
        "        # break\n",
        "    print(sum(epoch_loss)/len(epoch_loss))\n",
        "    training_losses.append(sum(epoch_loss)/len(epoch_loss))\n",
        "    \n",
        "     "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 798/798 [01:05<00:00, 12.21it/s]\n",
            "  0%|          | 2/798 [00:00<01:04, 12.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.10653062419671762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 798/798 [01:05<00:00, 12.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.062275560848172144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvG9fjHC1IEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(text):\n",
        "    # Vectorize and Pad.\n",
        "    vectorized_sent = toxic_data.vectorize(word_tokenize(text)).to(device)\n",
        "    pad_dim = (0, toxic_data.max_len - len(vectorized_sent))\n",
        "    vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')\n",
        "    # Forward Propagation.\n",
        "    # Unsqueeze because model is expecting `batch_size` x `sequence_len` shape.\n",
        "    outputs = model(vectorized_sent.unsqueeze(0)).squeeze()\n",
        "    # To get the boolean output, we check if outputs are > 0.5\n",
        "    return [int(l > 0.5) for l in outputs]\n",
        "    # What happens if you use torch.max instead? =)\n",
        "    # return label_column_names[int(torch.max(outputs, dim=1).indices)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM5ReEv51IE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c25fc189-acfa-4d1a-caf7-aea067fed00a"
      },
      "source": [
        "text = df_train.iloc[1017]['comment_text']\n",
        "print('predict outcome:')\n",
        "print([n for n in label_column_names if predict(text)[label_column_names.index(n)]==1])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict outcome:\n",
            "['toxic', 'obscene', 'insult']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVsbVzGh1IF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "630d7379-d834-41e6-c72e-5a476287fb38"
      },
      "source": [
        "print('test outcome:')\n",
        "print(df_train.iloc[1017]['comment_text'])\n",
        "print(df_train.iloc[1017])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test outcome:\n",
            "WOULDN'T BE THE FIRST TIME BITCH. FUCK YOU I'LL FIND OUT WHERE YOU LIVE, SODOMIZE YOUR WIFE AND THEN BURN YOUR HOUSE DOWN. FUCK YOU YOU FUCKING QUEER.\n",
            "id                                                         02c6e41e4b317ac3\n",
            "comment_text              WOULDN'T BE THE FIRST TIME BITCH. FUCK YOU I'L...\n",
            "toxic                                                                     1\n",
            "severe_toxic                                                              1\n",
            "obscene                                                                   1\n",
            "threat                                                                    1\n",
            "insult                                                                    1\n",
            "identity_hate                                                             1\n",
            "comment_text_tokenzied    [WOULD, N'T, BE, THE, FIRST, TIME, BITCH, ., F...\n",
            "Name: 1017, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}